import time
import tensorflow as tf
tf.python.control_flow_ops = tf

import numpy as np
import pandas as pd
import preprocess
from preprocess import Preprocess

import cv2
from sklearn.utils import shuffle
from scipy.misc import imread, imsave

from model import get_model
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
from keras.models import model_from_json
from keras.callbacks import LambdaCallback
import argparse
import random

def shadow(image):
	h, w = image.shape[0], image.shape[1]
	[x1, x2] = np.random.choice(w, 2, replace=False)
	k = h / (x2 - x1)
	b = - k * x1
	shadow = 0.5 #(np.random.random_sample() * 0.1 + 0.3)
	side = random.choice([True, False])
	shadow_image = np.array(image)
	for hi in range(h):
		c = int((hi - b) / k)
		if side:
			shadow_image[hi, c:, :] = (image[hi, c:, :] * shadow).astype(np.uint8)
		else:
			shadow_image[hi, :c, :] = (image[hi, :c, :] * shadow).astype(np.uint8)
	return shadow_image

# GENERATOR
# 
# yield images and steering angles for training and validation
# each entry in the log contains the information in the CSV file
# generated by the simulator and 3 extra parameters:
#
# - steering_bias_left:  amount of steering compensation for L camera
# - steering_bias_right: amount of steering compensation for R camera
# - augment_transforms:  string containing allowed transformations:
#
#   cf  = center flipped
#   l   = left camera
#   r   = right camera
#   lf  = left camera flipped
#   rf  = right camera flipped
# 
# In addition for each augmented image another one is created by applying
# a random shadow 
#
# So if an entry has 'cf l r lf rf' augment_transforms a total of
# 12 images will be generated (center + cf cf l r lf rf) and 6 more for their
# shadowed counterparts 
#  
def generator(log, validation = False):
	
	save_counter = 0
	
	while True:
		log = shuffle(log)

		if validation == False:
			print("Generator loop") # print to make sure we start yielding accross each epoch (debug only)

		for i, ll in log.groupby(np.arange(len(log)) // BS):

			# incoming size of images is 160x320
			images              = np.empty([0, 160, 320, 3], dtype=np.uint8)
			augmented_steerings = np.empty([0, 1], dtype=np.float32)

			for j,l in ll.iterrows():
				center = imread(l.center)
				st = l.steering

				images 				= np.vstack((images, [center]))
				augmented_steerings = np.vstack((augmented_steerings, [st]))

				if validation == False:
					images 				= np.vstack((images, [shadow(center)]))
					augmented_steerings = np.vstack((augmented_steerings, [st]))

					for augment_transform in l.augment_transforms.split():
						flip = False
						if augment_transform == 'cf':
							st = -l.steering
							image               = cv2.flip(center, flipCode = 1)
							images 				= np.vstack((images, [image], [shadow(image)]))
							augmented_steerings = np.vstack((augmented_steerings, [st], [st]))
						elif augment_transform.startswith('r'):
							st = l.steering + l.steering_bias_right
							if augment_transform.endswith('f'):
								st = -st
								flip = True
							right = imread(l.right)
							image = right if not flip else cv2.flip(right, flipCode = 1)
							images 				= np.vstack((images, [image], [shadow(image)]))
							augmented_steerings = np.vstack((augmented_steerings, [st], [st]))
						elif augment_transform.startswith('l'):
							st = l.steering + l.steering_bias_left
							if augment_transform.endswith('f'):
								st = -st
								flip = True
							left = imread(l.left)			
							image = left if not flip else cv2.flip(left, flipCode = 1)
							images 				= np.vstack((images, [image], [shadow(image)]))
							augmented_steerings = np.vstack((augmented_steerings, [st], [st]))

			images_processed = np.empty([0, Preprocess.sizey, Preprocess.sizex, 3], dtype=np.uint8)

			save_counter += 1
			for inum,im in enumerate(images):
				if (save_counter % 1000) == 0:
					imsave( "train-{}.jpg".format(inum), im)
				images_processed = np.vstack((images_processed, [Preprocess.preprocess(im)]))

			(images_processed, augmented_steerings) = shuffle(images_processed, augmented_steerings)

			yield (images_processed, np.clip(augmented_steerings, -1., 1.))

# balances dataset: take as much as bin_n items for each bin
# it doesnt take into account the augmented steerings 
def balance(log):
	balanced = pd.DataFrame()   # Balanced dataset
	bins =  1000                 # N of bins
	bin_n = 200                 # N of examples to include in each bin (at most)

	start = 0
	for end in np.linspace(0, 1, num=bins):  
		df_range = log[(np.absolute(log.steering) >= start) & (np.absolute(log.steering) < end)]
		range_n = min(bin_n, df_range.shape[0])
		if range_n > 0:
			balanced = pd.concat([balanced, df_range.sample(range_n)])
		start = end
	return balanced

# ***** main loop *****

if __name__ == "__main__":

	BS = 2

	parser = argparse.ArgumentParser(description='Train behavioral cloning udacity CarND P3')
	parser.add_argument('centerdir', type=str, default='driving-centered', help='Directory name of training data for CENTERED driving')
	parser.add_argument('leftdir', type=str, default='driving-left', help='Directory name of training data for driving on the LEFT')
	parser.add_argument('rightdir', type=str, default='driving-right', help='Directory name of training data for driving on the RIGHT')
	parser.add_argument('model', type=str, default="comma", help='Model (nvidia, comma)')
	args = parser.parse_args()

	current_model = args.model
	print('Using model: ',current_model)

	# Train the model
	# History is a record of training loss and metrics
	center_log = pd.read_csv(args.centerdir+'/driving_log.csv')
	left_log   = pd.read_csv(args.leftdir+'/driving_log.csv')
	right_log  = pd.read_csv(args.rightdir+'/driving_log.csv')
	
	sk_right_dir = 'driving-skewed-right-15'
	sk_right_log  = pd.read_csv(sk_right_dir+'/driving_log.csv')

	sk_left_dir = 'driving-skewed-left-15'
	sk_left_log  = pd.read_csv(sk_left_dir+'/driving_log.csv')
	
	for i in ['left', 'right', 'center']:
		center_log[i]   = args.centerdir + '/' + center_log[i].str.strip()
		left_log[i]     = args.leftdir   + '/' + left_log[i].str.strip()
		right_log[i]    = args.rightdir  + '/' + right_log[i].str.strip()
		sk_right_log[i] = sk_right_dir   + '/' + sk_right_log[i].str.strip()
		sk_left_log[i]  = sk_left_dir    + '/' + sk_left_log[i].str.strip()

	steering_bias = 0.2

	# driving in the center, steering follows the road
	#
	center_log['steering_bias_left']  =  steering_bias
	center_log['steering_bias_right'] = -steering_bias
	center_log['augment_transforms'] = "cf l r lf rf" 

	# driving at the left edge of the road, steering follows the road
	#
	left_log.steering  = left_log.steering + 0.7
	left_log['steering_bias_left']  =   steering_bias
	left_log['steering_bias_right'] =  -steering_bias
	left_log['augment_transforms'] = "cf r rf"

	# driving at the right edge of the road, steering follows the road
	#
	right_log.steering = right_log.steering - 0.7
	right_log['steering_bias_left']  =   steering_bias
	right_log['steering_bias_right'] =  -steering_bias
	right_log['augment_transforms'] = "cf l lf"
	
	# fragments of attempting to drive out of the road, pointing ~15 deg to the right
	# steering is 0. 
	sk_right_log.steering -= 0.6
	sk_right_log['steering_bias_left'] =   steering_bias
	sk_right_log['steering_bias_right'] = -steering_bias
	sk_right_log['augment_transforms'] = "cf r rf l lf"

	# fragments of attempting to drive out of the road, pointing ~15 deg to the left
	# steering is 0
	sk_left_log.steering += 0.6
	sk_left_log['steering_bias_left'] =   steering_bias
	sk_left_log['steering_bias_right'] = -steering_bias
	sk_left_log['augment_transforms'] = "cf l lf r rf"

	driving_log = pd.concat([center_log, left_log, right_log, sk_right_log, sk_left_log])
	print(np.histogram(np.abs(driving_log.steering), bins=100))

	print((driving_log.augment_transforms.str.split().str.len()+1).sum())

	train_log, validate_log = train_test_split(driving_log, test_size=0.20)
	print("Samples in train set:", train_log.shape[0])

	cb   = ModelCheckpoint(current_model + ".h5", monitor='val_loss', save_best_only=False)

	model = get_model(current_model)

	with open(current_model + ".json", "w") as file:
		file.write(model.to_json())

	history = model.fit_generator(generator(train_log), 
				nb_epoch=180, 
				samples_per_epoch = 2*(train_log.augment_transforms.str.split().str.len()+1).sum(),
				validation_data = generator(validate_log, validation=True),
				nb_val_samples = validate_log.shape[0],
				callbacks = [cb])

